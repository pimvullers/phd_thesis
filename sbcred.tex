\chapter{Self-Blindable Credentials}

The drawback of using regular public-key certificates, like X.509
certificates~\cite{ISO9594-8}, is that they are inherently traceable. This is
caused by the public key and signature that are contained in the certificate.
The public key can be used as a unique identifier for the user and is included
in the signature, making that identifying as well.

To circumvent this problem Verheul~\cite{Verheul01} proposes a
variant~\cite{BonehLS01,BonehLS04} of the Chaum-Pedersen signature
scheme~\cite{ChaumPedersen93} which allows these values to be randomised, or
\emph{blinded}, such that they are no longer traceable, while the signature can
still be verified. This signature scheme can then be used to implement a
credential system which allows the users themselves to blind their credentials
in order to prevent traceability.

In this chapter we describe the signature schemes and the resulting credential
system as well as our implementations~\cite{BatinaHJMV10,HoepmanJV10}.

\section{Self-Blindable Signatures}


\subsection{Chaum-Pedersen Signature Scheme}

Chaum and Pedersen~\cite{ChaumPedersen93} describe a basic signature scheme
which is intended to be used in combination with smart cards. This scheme works
in the DL setting and will be used as the basis for the following schemes.

The public key is a value $h$ together with a description of the prime-order
group in which the computations take place. As an example we use $(p, q, g)$,
which denotes a subgroup of $\Z*_p$ of prime-order $q$ with generator $g$. The
corresponding secret key is $x = \log_g h$.

\begin{algorithm}
  \caption{Generate a Chaum-Pedersen signature.}
  \label{alg:CP-sign}
  \addtolength{\baselineskip}{1mm}
  \begin{algorithmic}[1]
    \Function{CP-sign}{$m, (p, q, g), x$}
      \State $z \gets m^x \mod p$

      \State $a \gets g^s \mod p$
      \State $b \gets m^s \mod p$

      \State $c \gets \Call{Hash}{m, z, a, b}$

      \State $r \gets s + c \cdot x \mod q$

      \Return $(z, a, b, r)$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

The signature over a message $m$ is a single exponentiation with the private key
$z = m^x \mod p$ together with a proof that $\log_g h = \log_m z$. This
signature can be generated using Algorithm~\ref{alg:CP-sign}. Verification of
the signature consists of checking the proof according to
Algorithm~\ref{alg:CP-verify}. When the proof is correct, the verifier is
assured that the message is signed using the private key corresponding to the
public key used for verification.

\begin{algorithm}
  \caption{Verify a Chaum-Pedersen signature.}
  \label{alg:CP-verify}
  \addtolength{\baselineskip}{1mm}
  \begin{algorithmic}[1]
    \Function{CP-verify}{$m, (z, a, b, r), (p, q, g), h$}
      \State $c \gets \Call{Hash}{m, z, a, b}$

      \If{$g^r \neq a \cdot h^c \mod p$}
      \Return \Call{Invalid}{}
      \EndIf

      \If{$m^r \neq b \cdot z^c \mod p$}
      \Return \Call{Invalid}{}
      \EndIf

      \Return \Call{Valid}{}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\subsection{Boneh-Lynn-Shacham Signature Scheme}

Since the signature scheme of Chaum and Pedersen operates in the DL setting it
can also be used with elliptic curve cryptography. Boneh, Lynn and
Shacham~\cite{BonehLS01,BonehLS04} present a signature scheme that works on
elliptic curves with pairings and resembles the scheme by Chaum and Pedersen,
but omits the equality proof in order to achieve short signatures.

At the same time Verheul~\cite{Verheul01} gives a similar description of this
proofless variant of the Chaum-Pedersen scheme for groups in which the DDH
problem is easy while the DH and DL problems are hard. Elliptic curves with
pairings provide such groups. The proof of equality can in this case be
substituted by a pairing equation to be checked by the verifier.

The public key is a point $Q = x \cdot P_1$ on the curve together with a
description of the elliptic curve. The private key is the scalar value $x$. To
sign a message $m$ the signer transforms this into a point $P_m$ which is simply
multiplied with the private key as described in Algorithm~\ref{alg:BLS-sign}.

\begin{algorithm}
  \caption{Generate a Boneh-Lynn-Shacham signature.}
  \label{alg:BLS-sign}
  \addtolength{\baselineskip}{1mm}
  \begin{algorithmic}[1]
    \Function{BLS-sign}{$m, E, x$}
      \State $P_m \gets \Call{HashToPoint}{E, m}$
      \State $Z \gets x \cdot P_m$

      \Return $Z$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

To verify the signature the verifier has to perform the same check as with the
Chaum-Pedersen scheme, that is, whether the private key used to generate the
signature corresponds to the public key. This can be done using the following
equation:
\begin{equation}\label{eqn:BLS-verify}
  e(P_m, Q) = e(P_m, x \cdot P_2) = e(x \cdot P_m, P_2) = e(Z, P_2)
\end{equation}

\begin{algorithm}
  \caption{Verify a Boneh-Lynn-Shacham signature.}
  \label{alg:BLS-verify}
  \addtolength{\baselineskip}{1mm}
  \begin{algorithmic}[1]
    \Function{BLS-verify}{$m, Z, E, Q$}
      \State $P_m \gets \Call{HashToPoint}{E, m}$
      \If{$e(P_m, Q) \neq e(Z, P_2)$}
      \Return \Call{Invalid}{}
      \EndIf

      \Return \Call{Valid}{}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Verheul~\cite{Verheul01} points out a powerful aspect about these signatures,
they are invariant under blinding. When this signature is used to sign points on
the curve there is no need to transform the message to a point, such that the
\textsc{HashToPoint} operation can be omitted. Now the user can choose an
arbitrary number $b$ as blinding factor, and multiply both the message and
signature with this factor. The resulting pair
$(b \cdot P_m, b \cdot (x \cdot P_m))$ can still be verified using the
verification equation (\ref{eqn:BLS-verify}):
$$e(b \cdot P_m, Q) = e(b \cdot P_m, x \cdot P_2) =
e(b \cdot x \cdot P_m, P_2) = e(b \cdot Z, P_2)$$
Hence the signature remains valid. Since the user can perform this blinding all
by itself Verheul calls these signatures \emph{self-blindable}.

\section{Verheul's Self-Blindable Certificates}

Verheul proposes to use the self-blindable signatures to construct public-key
certificates which allow the users to randomise their public key pair and the
corresponding certificate. Such certificates can be used to circumvent
traceability based on the public key and signature. A certificate of a user
public key $P_U$ from an identity provider with public verification key $P_{ID}$
takes the form
\begin{equation}\label{eqn:SB-certificate}
  \{ P_U, Sig(P_U, s_{ID}) \}\text,
\end{equation}
where $s_{ID}$ is the private signing key of the identity provider corresponding
to $P_{ID}$.

\subsection{Credential Certificates}

Credential certificates are digital certificates that bind credentials to users
known by a public key. Proof of credential possession is given by proving
possession of the private key related to the public key referenced in the
certificate.

$\{P_U, [Sig(P_U, s_C), Cert(P_C, "Credential Issuer statement)]\}$

Here, the public key $P_U$ of the user is signed using a private key $s_C$ of
the credential issuer. The corresponding public key $P_C$ of the credential
issuer is included in a (conventional PKI) certificate which contains the
statement corresponding to this public key.

When we consider only a single statement per public key, these certificates can
be stored in a public database and can be omitted from the credential, given
that the corresponding public key can be identified. This allows the credentials
to be compact which is beneficial for device with limited storage capacities,
like smart cards.

To summarise, a credential certificate will consist of
\begin{itemize}
  \item the user's public key $P_U$,
  \item a signature $Sig(P_U, s_{Cx}) = s_{Cx} \cdot P_U$ over the public key
    using the credential signing key $s_{Cx}$, and
  \item a reference to the credential statement $x$.
\end{itemize}
The database will then contain
\begin{itemize}
  \item a reference $x$,
  \item the public verification key $P_{Cx}$ corresponding to $s_{Cx}$, and
  \item the credential statement.
\end{itemize}

\noindent [TODO: switch to smart card context, this context should be described in the
introduction, and this should mainly be a reference]

\section{Credential Issuance}

\begin{figure}
  \centering
  \includegraphics[scale=.4]{mscs/sbc_issuance}
  \caption{Issuance of self-blindable credential certificates.}\label{fig:SBC-issuance}
\end{figure}

To obtain a credential from a credential issuer the cardholder must provide the
issuer with its public key and proof possession of the corresponding private key.
This public key can be signed by an identity provider in order to increase the
trust level of the system. When the credential issuer has verified the
authenticity of the user he will verify eligibility for the requested attribute.
Once this has been verified the issuer signs the user's public key using the
private key corresponding to the requested attribute and send this signature to
the user.

\section{Credential Verification}

\begin{figure}
  \centering
  \includegraphics[scale=.4]{mscs/sbc_verification}
  \caption{Protocol for proving self-blindable attributes}\label{fig:textual_en}
\end{figure}

When the user wants to utilize an attribute stored in a credential it has to
show the credential to the service provider. To prevent the service provider
from tracing her based on the public key the user first blinds her key pair and
the credential. Next it sends the results from this blinding operation to the
service provider and proofs possession of the (blinded) private key. This last
step is easily achieved by signing a challenge received from the service
provider using this private key. The service provider can then verify this
signature using the public key it received earlier.


Of course, when a card presents a reader with such a pair $P_c$, $R$
the reader should not only check that $R$ is a proper signature on
$P_c$, that is, $R$ is $s\cdot P_c$, but also that the card knows the private
key corresponding to the public key $P_c$. This can be done via standard
challenge-response exchange, for example using ECDSA.

\section{Java Card Implementation}

To understand practical limitations and estimate performance of our protocols,
we implemented the protocol from \figurename~\ref{fig:basic} in Java
(terminal-side application) and Java Card (card-side application).
Our implementation involves the following components:
\begin{description}
\item[Bouncy Castle Library with Extension for Pairings] The Bouncy
  Castle (BC) library\footnote{\texttt{http://www.bouncycastle.org}} is a
  collection of cryptographic APIs for Java and C\# programming
  languages. Bouncy Castle provides full support for ECC and an
  interface to the common Java Cryptography Extension API. However, it
  does not implement pairings or elliptic curves over fields other
  than $\mathbb{F}_p$ and $\mathbb{F}_{2^m}$. Thus we have added our
  own implementations of $\mathbb{F}_{p^2}$ and $\mathbb{F}_{p^{12}}$,
  and the Tate, ate, and R-ate pairings.
This work greatly benefited
  from an ate pairing in Java that Paulo Barreto kindly made
  available, and algorithms Hankerson et al. published
  in~\cite{HankersonMS09}.
  To minimise maintenance overhead we strived
  to keep our extensions purely \emph{on top} of the Bouncy Castle
  library, that is, we did not change anything in the original
  library. In future we plan to contact the BC development team to
  incorporate our extensions into the official BC tree.
\item[Smart Card IO Library] Since version 6.0 the standard Java Development Kit
  includes support for communication with smart cards by providing the
  \texttt{javax.smartcardio} package. We used it to talk to a Java Card smart card
  on which our client applet was installed.
\item[A Java Card with the Client Applet] The protocol on the card-side is
  implemented as a Java Card~\cite{Chen00} applet and loaded onto
  a development Java Card.
\end{description}


\subsection{Available ECC Operations}

\subsubsection{EC Diffie-Hellman}

This functionality is provided by the javacard.security.KeyAgreement class. This
class can be instantiated using the getInstance(byte algorithm, boolean externalAccess)
method which returns a KeyAgreement object that implements a certain algorithm.
The generateSecret(byte[] publicData, short publicOffset, short publicLength, byte[] secret, short secretOffset)
method can then be used to actually perform the Diffie-Hellman operation, in our
case an elliptic curve multiplication.

The standard Java Card API provides two algorithms~\cite{JavaCard-2.2.2_API_documentation}:
\begin{description}
  \item[ALG\_EC\_SVDP\_DH] Elliptic curve secret value derivation primitive, Diffie-Hellman version, as per [IEEE P1363].
  \item[ALG\_EC\_SVDP\_DHC] Elliptic curve secret value derivation primitive, Diffie-Hellman version, with cofactor multiplication, as per [IEEE P1363].
\end{description}

Unfortunately for us, the implementation of these algorithms has two drawbacks:
\begin{enumerate}
  \item According to the IEEE P1363~\cite{IEEE_P1363} standard the shared secret computation by means of ECSVDP-DH and ECSVDP-DHC only returns the $x$-coordinate of the computed point.
  \item The Java Card implementation of these algorithms computes the SHA-1 message digest of the output of the derivation primitive to yeild a 20 byte result~\cite{JavaCard-2.2.2_API_documentation}.
\end{enumerate}

Especially this last transformation of the result prevents it from being useful for any further computation, other than using it as a secret key.

\paragraph{JCOP Extension}

Luckily the JCOP platform contains some extensions to the standard Java Card API.
In particular this extension provides the KeyAgreementX.ALG\_EC\_SVDP\_DH\_PLAIN algorithm:

\begin{lstlisting}
  /**
	 * KeyAgreement algorithm ALG_EC_SVDP_DH_PLAIN is the same as ALG_EC_SVDP_DH
	 * but without SHA1 postcomputation.
	 */
\end{lstlisting}

com.nxp.id.jcopx.KeyAgreementX
KeyAgreementX.getInstance(KeyAgreementX.ALG\_EC\_SVDP\_DH\_PLAIN, false);

This removes the second drawback as mentioned before and only leaves us with the
$x$-coordinate of the result instead of a point. However, the point can be reconstructed
from this coordinate using the elliptic curve formula. By inputting the $x$ value
we can compute the $y$ value. The only unknown is the sign of the $y$-coordinate,
hence we end up with two candidate points for the multiplication result.

\subsection{Public Key and Credential Blinding}

\begin{figure}
  \centering
  \includegraphics[scale=.4]{mscs/basic}
  \caption{Protocol for proving self-blindable attributes}\label{fig:basic}
\end{figure}

\subsection{Proof of Possession of the Private Key}

\begin{figure}
  \centering
  \includegraphics[scale=.4]{mscs/optimised}
  \caption{Protocol for proving self-blindable attributes}\label{fig:optimised}
\end{figure}

\subsubsection{Using the ECDSA Signature Scheme}

This approach depends on the NatLib library developed by Hendrik
Tews~\cite{TewsJacobs09}. This library is used to blind the secret key before
it can be used in the ECDSA algorithm to sign the challenge received from the
terminal.

\subsubsection{Using the BLS Signature Scheme}

This approach exploits the KeyGeneration functionality provided by the Java Card
API. This functionality is used to generate the blinding factor and blind the
received challenge at the same time, This allows us to simply sign the blinded
challenge with the private key using the BLS signature scheme.

\paragraph{Java Card Applet}\label{sec:applet}

In~\cite{BatinaHJMV10} the practical limitations of Java Cards have
been described that have to be taken into account while programming
the card. The actual operations that the card needs to perform are
scalar multiplication of points. In the end the applet performs the
required steps of the protocol in the following way.

The difficulty in the Java Card applet is the blinding of the private
key. This problem has been circumvented by (ab)using the EC key
generation operation to generate the blinding factor. This function
generates a random number which it multiplies with the generator point
of the elliptic curve. By setting the nonce, received from the
terminal, as the generator this function produces a private key (the
blinding factor $b$) and a public key (the blinded nonce ($b\cdot
(n\cdot P)$) which we can use for the remaining calculations.

Two EC Diffie-Hellman (ECDH) key agreement operations (effectively two scalar
point
multiplications) are performed with this generated private key to
calculate the blinded public key, and the blinded certificate. A third
ECDH key agreement is performed using the generated public key, the blinded
nonce, together with the cards private key to generate the signature.

Both operations, key generation and key agreement, use the
cryptographic coprocessor to perform the necessary calculations. This
improves performance since there are no longer calculations which have
to be done in software, which was the drawback from the original
implementation.

\paragraph{Terminal Application}\label{sec:application}

The terminal application needs to cope with the shortcomings of the Java Card
applet. This comes down to the fact that the terminal has to reconstruct the
points received from the card, $b\cdot P_c$ and $b\cdot C_a$, before they can
be processed any further.

If we know the $x$-coordinate of a point on the curve, the square of the
corresponding $y$-coordinate is known, namely as $y^{2} = x^{3} + ax + b$.
By taking the square root of $x^{3} + ax + b$ we find either $y$ or $-y$.
This forms the basis of ``point compression'', for compact representation
of points.  This is important for the implementation, because Diffie-Hellman
on a Java smart card only produces the $x$-coordinate of a multiplication,
as mentioned in Section~\ref{sec:applet}.

This reconstruction is a simple guess work, trying different signs for
the two $y$-coordinates. For the ECDSA signature verification this is not a
real issue since this verification is reasonably fast, although this is of
course not optimal. For the pairing signature verification simple guessing
is not desirable. Therefore we exploit the bilinearity of the pairing to
avoid computing more than two pairings, as would be the case without point
reconstruction.

First we calculate $e_1 = e(b\cdot P_c,\, Q_a)$ and $e_2 = e(b\cdot C_a,\, Q)$
where we take any sign for the $y$-coordinate of $b\cdot C_a$. If $e_1 = e_2$,
which happens if we have two right, or two wrong, signs in the first
parameters of the pairing, the verification succeeds. In the remaining case,
which means we took one right and one wrong sign, we check whether
$e_1\, e_2 = 1$ holds. If it holds, the verification also succeeds. This is
true because of the following. If $e_1 \neq e_2$, the error is caused by the
wrong sign resulting in one pairing being the inverse of the other, that is,
$e_2 = e_1^{-1}$. Here we can use that $e_1\, e_2 = e_1 (e_1^{-1}) = 1$ to
avoid an extra pairing calculation for the negated point of $b\cdot C_a$.


The implementation of the terminal application is not significantly
different from the original version. It only contains minor
modifications to incorporate the integration of the challenge-response
part in the first messages and the new signature check.

\section{Performance results}

\subsection{ECDSA using BigNat library}

The results of our tests are summarised in Table~\ref{tab:results_basic}. These
values are the average of ten test runs for each configuration, that is, for
each combination of key and blinding length.

\begin{table}
  \centering
  \caption{Test results of the original protocol for various key and blinding
lengths}\label{tab:results_basic}
  \renewcommand{\tabcolsep}{1.25mm}
  \renewcommand{\arraystretch}{1.25}
  \begin{tabular}{| c | c | c | c | c | c | c |}\hline

  key & blinding & attribute \& signature & verification & protocol &
communication \\
  (bits) & (bits) & (ms) & (ms) & (ms) & (bytes) \\\hline
  \hline

      & 192 & 2748 & 143 & 2891 & \\\cline{2-5}
  192 &  96 & 1884 & 136 & 2020 & 168 \\\cline{2-5}
      &  48 & 1451 & 130 & 1582 & \\\hline
  \hline

      & 160 & 1860 & 126 & 1987 & \\\cline{2-5}
  160 &  80 & 1355 & 133 & 1489 & 152 \\\cline{2-5}
      &  40 & 1113 & 127 & 1240 & \\\hline
  \hline

      & 128 & 1599 & 91 & 1691 & \\\cline{2-5}
  128 &  64 & 1143 & 93 & 1237 & 136 \\\cline{2-5}
      &  32 &  927 & 86 & 1014 & \\\hline
  \end{tabular}
\end{table}


The table shows the duration (in milliseconds) of the \textsf{getAttribute}
and \textsf{getSignature} requests to the card. The total amount of time
spent by the card, which is the sum of the durations of the requests, is
shown in the `card total' column. For the terminal we measured the duration
of the signature verifications, summarised in the `verification' column.
Finally the total duration of the protocol execution is shown in the
`protocol' column.

\subsection{Analysis}\label{sec:analysis}

We first look at the part of the protocol executed on the smart card. From the
results given in Table~\ref{tab:results_basic} it can be seen that the duration of
the \textsf{getAttribute} request depends only on the length of the key. This is in
strong contrast with the \textsf{getSignature} request which also depends heavily
on the blinding size.

This contrast can be explained by the available support from the
cryptographic coprocessor. For the blinding in the
\textsf{getAttribute} request the applet uses the ECDH primitive
provided by the coprocessor to perform the required two point
multiplications. The blinding in the \textsf{getSignature} request,
which requires a modular multiplication, has to be calculated
\emph{without} the help of the coprocessor.  In theory it is possible
to abuse the RSA cipher (and hence use the coprocessor) to do large
part of the modulo multiplication by using the fact that $4ab =
(a+b)^2 - (a-b)^2$, as in \cite{Sterckx09,TewsJacobs09}. The squares in this
equation can be performed by
doing an RSA encryption\slash exponentiation with a suitable RSA public
key, that is, one with the exponent 2 and the required modulus. The
numbers $a+b$ and $a-b$ are then just messages to be encrypted using
the RSA cipher, which is provided by the Java Card API.

We tried this approach, but with no success. The main obstacle is that
the RSA cipher on the card operates only within valid bit lengths for
RSA keys, starting with 512 bit keys. Although the number to be
multiplied (the message) can be any value, the number of non-zero bits
in the modulus has to be at least 488 bits for 512 bit keys according
to our tests. Since our modulus is only 192 bit long the card refused
to perform RSA encryption with such short modulus value. However, we
believe that a more flexible RSA implementation on the card would
allow this optimisation.

The performance of the terminal application is good, taking less than
a tenth of running time on the smart card. The drawback on this side is
caused by the use of the ECDH primitive to calculate the blinded value. This
results in the problem that the card only responds with the $x$-coordinates
of the blinded values. Therefore the terminal has to reconstruct the actual
point from these values as mentioned above. If the card could respond with
the actual points, either in compressed or uncompressed format, instead of
just the $x$-coordinate, the duration of the verification phase could be
shortened.

A large benefit of our use of ECC is the small amount of data that needs to be
exchanged between the terminal and the card. For key lengths of 192, 160 and 128
bits the total amount of bytes exchanged is 168, 152 and 136 respectively. This
would allow an implementation to use a \emph{single} APDU pair (command and
response) for all communication. This is in strong contrast with RSA-based
protocols~\cite{Sterckx09,TewsJacobs09} which already require multiple APDUs to
transfer a single command.


\subsection{BLS using key generation}

Using the same parameters as Batina et al.~\cite{BatinaHJMV10} we have
tested the performance of our optimised version of the protocol. The
results of these tests can be found in Table~\ref{tab:results_optimised}. The results of
the original protocol have been included, for comparison, in
Table~\ref{tab:results_basic}. Note
that we no longer use different blinding lengths since we now use the
key generator for this, which just uses the elliptic curve parameters.

\begin{table}
  \centering
  \caption{Test results of the optimised protocol for various key
    lengths}\label{tab:results_optimised}
  \renewcommand{\tabcolsep}{1.25mm}
  \renewcommand{\arraystretch}{1.25}
  \begin{tabular}{| c || c | c | c | c |}\hline
    key length & attribute \& signature & verification & protocol total &
      communication \\
    (bits) & (ms) & (ms) & (ms) & (bytes) \\\hline
    \hline
    192 & 787 & 116 & 904 & 155 \\\hline
    160 & 645 & 102 & 747 & 135 \\\hline
    128 & 535 &  82 & 617 & 115 \\\hline
  \end{tabular}
\end{table}

It can be seen that there is a significant increase in performance with respect
to the running times on the card from the original implementation. The smaller
amount of data which has to be exchanged during communication allowed us to
combine the protocol in a single command-response APDU pair, thus reducing the
amount of messages sent which also has a positive effect on the processing
overhead.

To get some more information about how the running time is spent on the card we
measured how long it takes to perform the individual operations. The results of
these measurements can be found in Table~\ref{tab:primitives}. The columns
indicate the time needed to perform a single operation. The processing overhead
is determined by subtracting one key generation and three key agreements from
the running time on the card.

\begin{table}
  \centering
  \caption{Test results for the API primitives}\label{tab:primitives}
  \renewcommand{\tabcolsep}{1.25mm}
  \renewcommand{\arraystretch}{1.25}
  \begin{tabular}{| c || c | c | c |}\hline
    key length & key generation & key agreement & processing overhead \\
    (bits) & (ms) & (ms) & (ms) \\\hline
    \hline
    192 & 379 & 98 & 114 \\\hline
    160 & 307 & 78 & 104 \\\hline
    128 & 242 & 62 & 107 \\\hline
  \end{tabular}
\end{table}

On the one hand, performing a scalar point multiplication (a key agreement) is
quite efficient, using less then 100 ms for the calculation. On the other hand,
performing a scalar point multiplication, combined with generating a random
value, (a key generation) is disappointing, taking more than a factor three
longer than just the multiplication. A possible explanation is a different
calculation which explains the fact that a key generation can return a complete
point, whereas a key agreement can only return the $x$-coordinate.

\section{MULTOS implementation??}

On the MULTOS platform the ECC API is also limited to ECDH, ECDSA and key generation.
We can, however, use the same trick as in the Java Card implementation to get the
BLS version to work. Furthermore the MULTOS API does provide ModularMultiplication
functionality which allows us to efficiently implement the ECDSA approach.

[Unfortunately the code does not run.]